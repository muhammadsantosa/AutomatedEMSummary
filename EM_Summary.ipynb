{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Frequency of Problem Codes and EM Breakdowns for Asset #\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#Reading CSV file and filtering the data out for RTG's (Type) and the Problem Codes that you dont want to include\n",
    "df = pd.read_csv(\"test.csv\")\n",
    "filter_type = input('Asset Type: ')\n",
    "df = df[df['Type']==filter_type] #Can be an input\n",
    "\n",
    "#Asset Maintenance Frequency\n",
    "asset_count = df['Asset #'].value_counts().to_dict()\n",
    "df2 = pd.DataFrame(asset_count.items(), columns =['asset', 'count'])\n",
    "df2 = df2.sort_values(by='count')\n",
    "others_sum = []\n",
    "for count in df2['count']:\n",
    "    if count <= 50:\n",
    "        others_sum.append(count)\n",
    "        df2.drop(df2[df2['count'] <= 50].index, inplace = True)\n",
    "others = pd.DataFrame({\"asset\": [\"Others\"], \"count\": [sum(others_sum)]})\n",
    "df2_1 = pd.concat([others, df2])\n",
    "\n",
    "plt.figure(1)\n",
    "plt.rcParams[\"figure.figsize\"] = (10,10)\n",
    "bar_width_1 = 1.5 \n",
    "spacing_factor_1 = 2\n",
    "total_width_1 = bar_width_1 + spacing_factor_1\n",
    "x_positions_1 = [i * total_width_1 for i in range(len(df2_1['asset']))]\n",
    "plt.bar(x_positions_1, df2_1['count'], width=bar_width_1, color='green')\n",
    "plt.xticks([pos + bar_width_1 / 2 for pos in x_positions_1], df2_1['asset'], rotation=70, fontsize=7)\n",
    "\n",
    "plt.xlabel('Asset #')\n",
    "plt.title('Frequency of ' + filter_type +' EM Breakdowns' )\n",
    "\n",
    "plt.savefig('Assets.png')\n",
    "\n",
    "#Problem Code Frequency\n",
    "pc_count = df['Problem\\r\\nCode'].value_counts().to_dict()\n",
    "df3 = pd.DataFrame(pc_count.items(), columns = ['pc', 'count'])\n",
    "df3 = df3.sort_values(by='count')\n",
    "\n",
    "plt.figure(2)\n",
    "bar_width_2 = 1.5 \n",
    "spacing_factor_2 = 2\n",
    "total_width_2 = bar_width_2 + spacing_factor_2\n",
    "x_positions_2 = [i * total_width_2 for i in range(len(df3['pc']))]\n",
    "plt.bar(x_positions_2, df3['count'], width=bar_width_2, color='green')\n",
    "plt.xticks([pos + bar_width_2 / 2 for pos in x_positions_2], df3['pc'], rotation=45, fontsize=7)\n",
    "\n",
    "plt.xlabel('Problem Code')\n",
    "plt.title('Frequency of Problem Code ' + filter_type +' EM Breakdowns')\n",
    "\n",
    "plt.savefig('Problem_Code.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Filtering Data Based on Input\n",
    "problem_filter = []\n",
    "list_length = 3\n",
    "for i in range(list_length):\n",
    "    n = i+1\n",
    "    problem = input('Which Problem Codes do you want a Summary of? Enter Problem Code ' + str(n) + \"/3 : \" )\n",
    "    problem_filter.append(problem)\n",
    "\n",
    "df1 = pd.DataFrame()\n",
    "for problem in problem_filter:\n",
    "    new_df = df[df[\"Problem\\r\\nCode\"] == problem]\n",
    "    df1 = pd.concat([df1, new_df], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Autocorrecting all descriptions and putting it into a new dataframe\n",
    "import spacy\n",
    "import enchant\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "spellchecker = enchant.Dict(\"en_US\")\n",
    "\n",
    "description = df['Description']\n",
    "work_log = df['Work Log']\n",
    "\n",
    "def autocorrect(text):\n",
    "    if isinstance(text, float):\n",
    "        return str(text)  # Convert float to string\n",
    "    \n",
    "    if not isinstance(text, str):\n",
    "        return text\n",
    "    doc = nlp(text)\n",
    "    corrected_text = []\n",
    "    for token in doc:\n",
    "        if not spellchecker.check(token.text):\n",
    "            suggestion = spellchecker.suggest(token.text)\n",
    "            if suggestion:\n",
    "                corrected_text.append(suggestion[0])\n",
    "            else:\n",
    "                corrected_text.append(token.text)\n",
    "        else:\n",
    "            corrected_text.append(token.text)\n",
    "\n",
    "    return \" \".join(corrected_text)\n",
    "\n",
    "corrected_description = df1['Description'].apply(lambda x: autocorrect(x))\n",
    "df4 = df1.replace(description, corrected_description)\n",
    "\n",
    "corrected_work_log = df4['Work Log'].apply(lambda x: autocorrect(x))\n",
    "df5 = df4.replace(work_log, corrected_work_log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Organizing into PC \n",
    "pc = df1['Problem\\r\\nCode'].unique()\n",
    "df6 = df5.loc[:,[\"Problem\\r\\nCode\",\"Cause Code\", \"Description\", \"Work Log\"]]\n",
    "dfs = []\n",
    "df7 = {}\n",
    "\n",
    "for code in pc:\n",
    "    new_df = df6[df6['Problem\\r\\nCode']==code].dropna()\n",
    "    dfs.append(new_df.sort_values(by = \"Cause Code\").reset_index(drop=True))\n",
    "\n",
    "for key, value in enumerate(dfs):\n",
    "    df7[pc[key]] = value\n",
    "    \n",
    "#Organizing into CC\n",
    "count_cc = []\n",
    "cause_cc = []\n",
    "cc = []\n",
    "dfn = []\n",
    "df8 = {}\n",
    "\n",
    "for code in pc:\n",
    "    ccdf = df7[code]\n",
    "    cc_count = ccdf[\"Cause Code\"].value_counts().to_dict()\n",
    "    count_cc.append(list(cc_count.values()))\n",
    "    cause_cc.append(list(cc_count.keys()))\n",
    "    for cause, count in cc_count.items():\n",
    "        if count > 100:#Could be an input, filter based on how many you think there should be\n",
    "            new_df = ccdf[ccdf['Cause Code']==cause]\n",
    "            dfn.append(new_df.sort_values(by = \"Description\").reset_index(drop=True))\n",
    "            cc.append(cause)\n",
    "        else:\n",
    "            pass\n",
    "\n",
    "for key, value in enumerate(dfn):\n",
    "    df8[cc[key]] = value\n",
    "\n",
    "image_paths = []\n",
    "\n",
    "for i in range(3): #Just the first three most common Problem Code Cause Code Graphs\n",
    "    plt.figure(i)\n",
    "    plt.rcParams[\"figure.figsize\"] = (10,10)\n",
    "    bar_width_1 = 1.5 \n",
    "    spacing_factor_1 = 2\n",
    "    total_width_1 = bar_width_1 + spacing_factor_1\n",
    "    x_positions_1 = [i * total_width_1 for i in range(len(cause_cc[i]))]\n",
    "    plt.bar(x_positions_1, count_cc[i], width=bar_width_1, color='green')\n",
    "    plt.xticks([pos + bar_width_1 / 2 for pos in x_positions_1], cause_cc[i], rotation=45, fontsize=8)\n",
    "    plt.xlabel('Cause Code')\n",
    "    plt.title(pc[i] + \": Cause Code Frequency Breakdowns\")\n",
    "\n",
    "    image_path = f\"plot_{i+1}.png\"\n",
    "    plt.savefig(image_path)\n",
    "    image_paths.append(image_path)\n",
    "    plt.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cohere\n",
    "co = cohere.Client(\"soyz0L4mhFJmfoFv3EyEZuGfq0jvcu5Jb9WF8Yba\")\n",
    "\n",
    "def summarize(text, co):\n",
    "    response = co.summarize(\n",
    "        text=text,\n",
    "        model='summarize-xlarge',\n",
    "        length='medium',\n",
    "        format='bullets',\n",
    "        additional_command='making it evident what the most common problems were and what was done to fix them, while using third person language',\n",
    "        temperature=2,\n",
    "        extractiveness= \"medium\",\n",
    "    )\n",
    "\n",
    "    summary = response.summary\n",
    "    return summary\n",
    "\n",
    "def chunk_text(text, max_chars):\n",
    "    chunks = []\n",
    "    current_chunk = \"\"\n",
    "\n",
    "    for word in text.split():\n",
    "        word_length = len(word)  # Length of the current word\n",
    "\n",
    "        if len(current_chunk) + word_length <= max_chars:\n",
    "            current_chunk += \" \" + word\n",
    "        else:\n",
    "            chunks.append(current_chunk.strip())\n",
    "            current_chunk = word\n",
    "\n",
    "    if current_chunk:\n",
    "        chunks.append(current_chunk.strip())\n",
    "\n",
    "    return chunks\n",
    "\n",
    "max_chars_per_chunk = 100000\n",
    "\n",
    "for cause in cc[:3]:\n",
    "    df = df8[cause]\n",
    "    work_log =df[\"Work Log\"]\n",
    "    text = \" \".join(work_log)\n",
    "    print(\"-------------------------\")\n",
    "    print(str(df['Problem\\r\\nCode'].unique()[0]) + \": \" + cause + \" Summary\")\n",
    "\n",
    "    text_chunks = chunk_text(text, max_chars_per_chunk)\n",
    "    for chunk in text_chunks:\n",
    "        chunk_summary = summarize(chunk, co)\n",
    "        print(chunk_summary)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
